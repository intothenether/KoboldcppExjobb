{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/intothenether/KoboldcppExjobb/blob/main/KoboldCpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FCn5tmpn3UV"
      },
      "source": [
        "## Welcome to the Official KoboldCpp Colab Notebook\n",
        "It's really easy to get started. Just press the two **Play** buttons below, and then connect to the **Cloudflare URL** shown at the end.\n",
        "You can select a model from the dropdown, or enter a **custom URL** to a GGUF model (Example: `https://huggingface.co/KoboldAI/LLaMA2-13B-Tiefighter-GGUF/resolve/main/LLaMA2-13B-Tiefighter.Q4_K_M.gguf`)\n",
        "\n",
        "**Keep this page open and occationally check for captcha's so that your AI is not shut down**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNaj3u0jn3UW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "4a3f6ebe-350a-4598-e6b1-08360ff21270"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<b>Press play on the music player to keep the tab alive, then start KoboldCpp below</b><br/>\n",
              "<audio autoplay=\"\" src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" loop controls>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title <-- Tap this if you play on Mobile { display-mode: \"form\" }\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start KoboldCpp below</b><br/>\n",
        "<audio autoplay=\"\" src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" loop controls>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5aNaZ7hbI2hm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJS9i_Dltv8Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50e33b0-e786-43e6-d082-387c3ab3cf3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to request access to save to your google drive...\n",
            "Mounted at /content/drive\n",
            "Created new koboldcpp_save_db.jsondb at /content/drive/MyDrive/koboldcpp_data/koboldcpp_save_db.jsondb\n",
            "/content\n",
            "Downloading KoboldCpp, please wait...\n",
            "--2025-07-28 17:57:08--  https://kcpplinux.concedo.workers.dev/\n",
            "Resolving kcpplinux.concedo.workers.dev (kcpplinux.concedo.workers.dev)... 172.67.145.201, 104.21.71.155, 2606:4700:3036::ac43:91c9, ...\n",
            "Connecting to kcpplinux.concedo.workers.dev (kcpplinux.concedo.workers.dev)|172.67.145.201|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/LostRuins/koboldcpp/releases/latest/download/koboldcpp-linux-x64 [following]\n",
            "--2025-07-28 17:57:08--  https://github.com/LostRuins/koboldcpp/releases/latest/download/koboldcpp-linux-x64\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/LostRuins/koboldcpp/releases/download/v1.96.2/koboldcpp-linux-x64 [following]\n",
            "--2025-07-28 17:57:09--  https://github.com/LostRuins/koboldcpp/releases/download/v1.96.2/koboldcpp-linux-x64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/614637892/538341fa-163f-4069-a018-329ae1954e8f?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-28T18%3A51%3A50Z&rscd=attachment%3B+filename%3Dkoboldcpp-linux-x64&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-28T17%3A51%3A22Z&ske=2025-07-28T18%3A51%3A50Z&sks=b&skv=2018-11-09&sig=PMX3o0GeRZ0O2WNOpLps8DEazuA22pB3HpAo2ep1d8Q%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MzcyNTcyOSwibmJmIjoxNzUzNzI1NDI5LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.mHFUkcA7UnmtJ1ux-1ppvWlPtBDD7Vf9ZtjUIt8Lz0c&response-content-disposition=attachment%3B%20filename%3Dkoboldcpp-linux-x64&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-07-28 17:57:09--  https://release-assets.githubusercontent.com/github-production-release-asset/614637892/538341fa-163f-4069-a018-329ae1954e8f?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-07-28T18%3A51%3A50Z&rscd=attachment%3B+filename%3Dkoboldcpp-linux-x64&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-07-28T17%3A51%3A22Z&ske=2025-07-28T18%3A51%3A50Z&sks=b&skv=2018-11-09&sig=PMX3o0GeRZ0O2WNOpLps8DEazuA22pB3HpAo2ep1d8Q%3D&jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1MzcyNTcyOSwibmJmIjoxNzUzNzI1NDI5LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.mHFUkcA7UnmtJ1ux-1ppvWlPtBDD7Vf9ZtjUIt8Lz0c&response-content-disposition=attachment%3B%20filename%3Dkoboldcpp-linux-x64&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 572058496 (546M) [application/octet-stream]\n",
            "Saving to: ‘dlfile.tmp’\n",
            "\n",
            "dlfile.tmp          100%[===================>] 545.56M  82.8MB/s    in 4.5s    \n",
            "\n",
            "2025-07-28 17:57:13 (121 MB/s) - ‘dlfile.tmp’ saved [572058496/572058496]\n",
            "\n",
            "Download Successful\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,152 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,269 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,574 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,160 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,471 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,772 kB]\n",
            "Fetched 21.8 MB in 2s (10.3 MB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "42 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 1,513 kB of archives.\n",
            "After this operation, 5,441 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.3 [45.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaria2-0 amd64 1.36.0-1 [1,086 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 aria2 amd64 1.36.0-1 [381 kB]\n",
            "Fetched 1,513 kB in 2s (976 kB/s)\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "\n",
            "07/28 17:57:30 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "07/28 17:57:30 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cas-bridge.xethub.hf.co/xet-bridge-us/677e9a172e44cb75e02f268a/52579b79d3d203bd3bc0713c135c6552a7b8de2243b3f52fea0f388b5788d6da?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250728%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250728T175730Z&X-Amz-Expires=3600&X-Amz-Signature=7485e505de0ec4c1ccb80c179f5129dad951357621cde32340e8a4d508607d46&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27phi-4-Q6_K_L.gguf%3B+filename%3D%22phi-4-Q6_K_L.gguf%22%3B&x-id=GetObject&Expires=1753729050&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzcyOTA1MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NzdlOWExNzJlNDRjYjc1ZTAyZjI2OGEvNTI1NzliNzlkM2QyMDNiZDNiYzA3MTNjMTM1YzY1NTJhN2I4ZGUyMjQzYjNmNTJmZWEwZjM4OGI1Nzg4ZDZkYSoifV19&Signature=o0o4aEmWvxHdVxDv5CkvJphQj4IqilsymD1FnRAdAr0mm-9Z7nsJvD-zIl6hhViRf%7EGQHEUGrONg%7EJAmI9Lp2H4gsGNt-fOVG1eaRt6RFABrLV0sXiJUqFF28loRtUrR%7ECcSkbtbkqL5gpCen5ECKwTi7KuQW-DG747Yg61E0OZtLNkdCYwEZ8HLW1zXtdZwKjAbZXxToJk-ro5mTJniLWJRDGQLlZGO-MaWv-zMtgoCIbh1KwdBNkTG36UDUIBMuvSmz4inWPEtzbzKxaoVoANi3cvPFIKJ-bN8RredT%7E%7EHzuzQkLrI7xQn5WeaptlczGI2ahdadFfUQN7J9VT11Q__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "\n",
            "07/28 17:57:30 [\u001b[1;32mNOTICE\u001b[0m] CUID#8 - Redirecting to https://cas-bridge.xethub.hf.co/xet-bridge-us/677e9a172e44cb75e02f268a/52579b79d3d203bd3bc0713c135c6552a7b8de2243b3f52fea0f388b5788d6da?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250728%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250728T174842Z&X-Amz-Expires=3600&X-Amz-Signature=734cfbb768a8c189a90fa598005f41a905c8d88a13343aa25ce2a1347b9a8a89&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27phi-4-Q6_K_L.gguf%3B+filename%3D%22phi-4-Q6_K_L.gguf%22%3B&x-id=GetObject&Expires=1753728522&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzcyODUyMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NzdlOWExNzJlNDRjYjc1ZTAyZjI2OGEvNTI1NzliNzlkM2QyMDNiZDNiYzA3MTNjMTM1YzY1NTJhN2I4ZGUyMjQzYjNmNTJmZWEwZjM4OGI1Nzg4ZDZkYSoifV19&Signature=S6Wib5%7EOFx%7EdgdgNoFYtjfpmwEM1j5BB6qpnW-83v7PyeVEVQVb4cgxNiHVd%7EwExWdRHI-LN7gkHndFyrwwexz%7ERBTerxAYOPtNzSX7dnwEO18WLXgl8GsUvs42GCxjAvAk0XFnXjhA4PHBo-FDIk3Q0tMPT54ZV2h2sNKj5Nwh95Dprp-pj0jk7yu66Ku6Waae1LiZIqQ%7EZnY9BiBcbzmKd2O%7E%7EtwYYHHm-5GX8ki7eUEdqn7S2tgB7lfD9KEd8NISgE9mK6OfxlrcLg7mT466Ui1hrnA%7EbFjFjD9NTCijRbgCDiq4JQl9bTaFw2yPszj7-c5dn1bR1Os2Y2GpZmg__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "\n",
            "07/28 17:57:30 [\u001b[1;32mNOTICE\u001b[0m] CUID#9 - Redirecting to https://cas-bridge.xethub.hf.co/xet-bridge-us/677e9a172e44cb75e02f268a/52579b79d3d203bd3bc0713c135c6552a7b8de2243b3f52fea0f388b5788d6da?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250728%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250728T175431Z&X-Amz-Expires=3600&X-Amz-Signature=144f149d429971c74713c5578aacefc7c0035a69660275f113cc93b00ae1d8f5&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27phi-4-Q6_K_L.gguf%3B+filename%3D%22phi-4-Q6_K_L.gguf%22%3B&x-id=GetObject&Expires=1753728871&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzcyODg3MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NzdlOWExNzJlNDRjYjc1ZTAyZjI2OGEvNTI1NzliNzlkM2QyMDNiZDNiYzA3MTNjMTM1YzY1NTJhN2I4ZGUyMjQzYjNmNTJmZWEwZjM4OGI1Nzg4ZDZkYSoifV19&Signature=pewS3t2gvsFlTwSiMmIshpYcI4dLues%7EskIM2IlCihrjJdCf7Mybfk9DBudjYFDIMgaOpbyys8kn6wYvUH%7EHr-%7EA3J2fcaKzATQMKl7lfZleAs244mRsml7sfUJmbDdIf9jkI8%7ETCZ8cCBvLAvQ4HsIv2AumxmCY7hRKnC2b6cU12Kt42GEeWUa9NmOZCep%7EYHQUYcnihup9jCQjvi6BdBtcKTEjJcWNGph-1Swfm3lgofGIiFQmHPs2DGeSHmkVR6dNeC7K93ZWi6Mz6fq7pG0B811iszTk8RQsEuRmn1IRXMgvvOz4JsC-HP9aPmxdB%7E8aXpTdd0tAynx%7Et5AMaA__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "\n",
            "07/28 17:57:30 [\u001b[1;32mNOTICE\u001b[0m] CUID#10 - Redirecting to https://cas-bridge.xethub.hf.co/xet-bridge-us/677e9a172e44cb75e02f268a/52579b79d3d203bd3bc0713c135c6552a7b8de2243b3f52fea0f388b5788d6da?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250728%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250728T175730Z&X-Amz-Expires=3600&X-Amz-Signature=7485e505de0ec4c1ccb80c179f5129dad951357621cde32340e8a4d508607d46&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27phi-4-Q6_K_L.gguf%3B+filename%3D%22phi-4-Q6_K_L.gguf%22%3B&x-id=GetObject&Expires=1753729050&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzcyOTA1MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NzdlOWExNzJlNDRjYjc1ZTAyZjI2OGEvNTI1NzliNzlkM2QyMDNiZDNiYzA3MTNjMTM1YzY1NTJhN2I4ZGUyMjQzYjNmNTJmZWEwZjM4OGI1Nzg4ZDZkYSoifV19&Signature=o0o4aEmWvxHdVxDv5CkvJphQj4IqilsymD1FnRAdAr0mm-9Z7nsJvD-zIl6hhViRf%7EGQHEUGrONg%7EJAmI9Lp2H4gsGNt-fOVG1eaRt6RFABrLV0sXiJUqFF28loRtUrR%7ECcSkbtbkqL5gpCen5ECKwTi7KuQW-DG747Yg61E0OZtLNkdCYwEZ8HLW1zXtdZwKjAbZXxToJk-ro5mTJniLWJRDGQLlZGO-MaWv-zMtgoCIbh1KwdBNkTG36UDUIBMuvSmz4inWPEtzbzKxaoVoANi3cvPFIKJ-bN8RredT%7E%7EHzuzQkLrI7xQn5WeaptlczGI2ahdadFfUQN7J9VT11Q__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "\n",
            "07/28 17:57:30 [\u001b[1;32mNOTICE\u001b[0m] CUID#11 - Redirecting to https://cas-bridge.xethub.hf.co/xet-bridge-us/677e9a172e44cb75e02f268a/52579b79d3d203bd3bc0713c135c6552a7b8de2243b3f52fea0f388b5788d6da?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250728%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250728T175730Z&X-Amz-Expires=3600&X-Amz-Signature=7485e505de0ec4c1ccb80c179f5129dad951357621cde32340e8a4d508607d46&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27phi-4-Q6_K_L.gguf%3B+filename%3D%22phi-4-Q6_K_L.gguf%22%3B&x-id=GetObject&Expires=1753729050&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzcyOTA1MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NzdlOWExNzJlNDRjYjc1ZTAyZjI2OGEvNTI1NzliNzlkM2QyMDNiZDNiYzA3MTNjMTM1YzY1NTJhN2I4ZGUyMjQzYjNmNTJmZWEwZjM4OGI1Nzg4ZDZkYSoifV19&Signature=o0o4aEmWvxHdVxDv5CkvJphQj4IqilsymD1FnRAdAr0mm-9Z7nsJvD-zIl6hhViRf%7EGQHEUGrONg%7EJAmI9Lp2H4gsGNt-fOVG1eaRt6RFABrLV0sXiJUqFF28loRtUrR%7ECcSkbtbkqL5gpCen5ECKwTi7KuQW-DG747Yg61E0OZtLNkdCYwEZ8HLW1zXtdZwKjAbZXxToJk-ro5mTJniLWJRDGQLlZGO-MaWv-zMtgoCIbh1KwdBNkTG36UDUIBMuvSmz4inWPEtzbzKxaoVoANi3cvPFIKJ-bN8RredT%7E%7EHzuzQkLrI7xQn5WeaptlczGI2ahdadFfUQN7J9VT11Q__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            " *** Download Progress Summary as of Mon Jul 28 17:57:35 2025 *** \n",
            "=\n",
            "[#dc52d3 0.9GiB/11GiB(7%) CN:5 DL:183MiB ETA:58s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Mon Jul 28 17:57:40 2025 *** \n",
            "=\n",
            "[#dc52d3 1.7GiB/11GiB(14%) CN:5 DL:150MiB ETA:1m6s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Mon Jul 28 17:57:46 2025 *** \n",
            "=\n",
            "[#dc52d3 2.1GiB/11GiB(19%) CN:5 DL:110MiB ETA:1m25s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Mon Jul 28 17:57:51 2025 *** \n",
            "=\n",
            "[#dc52d3 2.8GiB/11GiB(24%) CN:5 DL:94MiB ETA:1m33s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Mon Jul 28 17:57:57 2025 *** \n",
            "=\n",
            "[#dc52d3 3.8GiB/11GiB(33%) CN:5 DL:172MiB ETA:44s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Mon Jul 28 17:58:02 2025 *** \n",
            "=\n",
            "[#dc52d3 4.5GiB/11GiB(39%) CN:5 DL:134MiB ETA:52s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Mon Jul 28 17:58:08 2025 *** \n",
            "=\n",
            "[#dc52d3 4.9GiB/11GiB(43%) CN:5 DL:100MiB ETA:1m5s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Mon Jul 28 17:58:14 2025 *** \n",
            "=\n",
            "[#dc52d3 5.6GiB/11GiB(49%) CN:5 DL:110MiB ETA:53s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Mon Jul 28 17:58:20 2025 *** \n",
            "=\n",
            "[#dc52d3 5.9GiB/11GiB(52%) CN:5 DL:100MiB ETA:55s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Mon Jul 28 17:58:25 2025 *** \n",
            "=\n",
            "[#dc52d3 6.5GiB/11GiB(57%) CN:5 DL:99MiB ETA:49s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Mon Jul 28 17:58:31 2025 *** \n",
            "=\n",
            "[#dc52d3 7.6GiB/11GiB(66%) CN:5 DL:168MiB ETA:23s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Mon Jul 28 17:58:36 2025 *** \n",
            "=\n",
            "[#dc52d3 8.6GiB/11GiB(75%) CN:5 DL:197MiB ETA:14s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Mon Jul 28 17:58:42 2025 *** \n",
            "=\n",
            "[#dc52d3 9.5GiB/11GiB(83%) CN:5 DL:169MiB ETA:11s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Mon Jul 28 17:58:47 2025 *** \n",
            "=\n",
            "[#dc52d3 10GiB/11GiB(90%) CN:5 DL:188MiB ETA:5s]\n",
            "FILE: /content/model.gguf\n",
            "-\n",
            "\n",
            "\u001b[0m\n",
            "07/28 17:58:52 [\u001b[1;32mNOTICE\u001b[0m] Download complete: /content/model.gguf\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "dc52d3|\u001b[1;32mOK\u001b[0m  |   143MiB/s|/content/model.gguf\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "***\n",
            "Welcome to KoboldCpp - Version 1.96.2\n",
            "Downloading https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Attempting to start tunnel thread...\n",
            "Loading Chat Completions Adapter: /tmp/_MEIDqTWu6/kcpp_adapters/AutoGuess.json\n",
            "Chat Completions Adapter Loaded\n",
            "Loaded existing savedatafile at '/content/drive/MyDrive/koboldcpp_data/koboldcpp_save_db.jsondb'.\n",
            "System: Linux #1 SMP PREEMPT_DYNAMIC Sun Mar 30 16:01:29 UTC 2025 x86_64 x86_64\n",
            "Detected Available GPU Memory: 15360 MB\n",
            "Unable to determine available RAM\n",
            "Initializing dynamic library: koboldcpp_cublas.so\n",
            "Starting Cloudflare Tunnel for Linux, please wait...\n",
            "==========\n",
            "Namespace(admin=False, admindir='', adminpassword=None, analyze='', benchmark=None, blasbatchsize=512, blasthreads=1, chatcompletionsadapter='AutoGuess', cli=False, config=None, contextsize=4096, debugmode=0, defaultgenamt=512, draftamount=8, draftgpulayers=999, draftgpusplit=None, draftmodel='', embeddingsgpu=False, embeddingsmaxctx=0, embeddingsmodel='', enableguidance=False, exportconfig='', exporttemplate='', failsafe=False, flashattention=True, forceversion=0, foreground=False, gpulayers=99, highpriority=False, hordeconfig=None, hordegenlen=0, hordekey='', hordemaxctx=0, hordemodelname='', hordeworkername='', host='', ignoremissing=False, launch=False, lora=None, loramult=1.0, maingpu=-1, maxrequestsize=32, mmproj='', mmprojcpu=False, model=[], model_param='model.gguf', moeexperts=-1, multiplayer=False, multiuser=1, noavx2=False, noblas=False, nobostoken=False, nocertify=False, nofastforward=False, nommap=False, nomodel=False, noshift=False, onready='', overridekv='', overridetensors='', password=None, port=5001, port_param=5001, preloadstory='', prompt='', promptlimit=100, quantkv=0, quiet=True, remotetunnel=True, ropeconfig=[0.0, 10000.0], savedatafile='/content/drive/MyDrive/koboldcpp_data/koboldcpp_save_db.jsondb', sdclamped=0, sdclampedsoft=0, sdclipg='', sdclipl='', sdconfig=None, sdlora='', sdloramult=1.0, sdmodel='', sdnotile=False, sdphotomaker='', sdquant=False, sdt5xxl='', sdthreads=0, sdtiledvae=768, sdvae='', sdvaeauto=False, showgui=False, singleinstance=False, skiplauncher=False, smartcontext=False, ssl=None, tensor_split=None, threads=1, ttsgpu=False, ttsmaxlen=4096, ttsmodel='', ttsthreads=0, ttswavtokenizer='', unpack='', useclblast=None, usecpu=False, usecuda=['0', 'mmq'], usemlock=False, usemmap=False, useswa=False, usevulkan=None, version=False, visionmaxres=1024, websearch=True, whispermodel='')\n",
            "==========\n",
            "Loading Text Model: /content/model.gguf\n",
            "\n",
            "The reported GGUF Arch is: phi3\n",
            "Arch Category: 0\n",
            "\n",
            "---\n",
            "Identified as GGUF model.\n",
            "Attempting to Load...\n",
            "---\n",
            "Using automatic RoPE scaling for GGUF. If the model has custom RoPE settings, they'll be used directly instead!\n",
            "System Info: AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "CUDA MMQ: True\n",
            "---\n",
            "Initializing CUDA/HIP, please wait, the following step may take a few minutes (only for first launch)...\n",
            "---\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 37 key-value pairs and 243 tensors from /content/model.gguf (version GGUF V3 (latest))\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_0\n",
            "print_info: file size   = 11.43 GiB (6.70 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: special tokens cache size = 96\n",
            "load: token to piece cache size = 0.6151 MB\n",
            "print_info: arch             = phi3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 16384\n",
            "print_info: n_embd           = 5120\n",
            "print_info: n_layer          = 40\n",
            "print_info: n_head           = 40\n",
            "print_info: n_head_kv        = 10\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1280\n",
            "print_info: n_embd_v_gqa     = 1280\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 17920\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 250000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 16384\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 14B\n",
            "print_info: model params     = 14.66 B\n",
            "print_info: general.name     = Phi 4\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 100352\n",
            "print_info: n_merges         = 100000\n",
            "print_info: BOS token        = 100257 '<|endoftext|>'\n",
            "print_info: EOS token        = 100265 '<|im_end|>'\n",
            "print_info: EOT token        = 100265 '<|im_end|>'\n",
            "print_info: PAD token        = 100257 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: FIM PRE token    = 100258 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 100260 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 100259 '<|fim_middle|>'\n",
            "print_info: EOG token        = 100257 '<|endoftext|>'\n",
            "print_info: EOG token        = 100265 '<|im_end|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = false)\n",
            "load_tensors: relocated tensors: 0 of 243\n",
            "load_tensors: offloading 40 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 41/41 layers to GPU\n",
            "load_tensors:    CUDA_Host model buffer size =   520.62 MiB\n",
            "load_tensors:        CUDA0 model buffer size = 11186.27 MiB\n",
            "load_all_data: using async uploads for device CUDA0, buffer type CUDA0, backend CUDA0\n",
            "..................................................................................\n",
            "Automatic RoPE Scaling: Using model internal value.\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4224\n",
            "llama_context: n_ctx_per_seq = 4224\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 1\n",
            "llama_context: kv_unified    = true\n",
            "llama_context: freq_base     = 250000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4224) < n_ctx_train (16384) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:  CUDA_Host  output buffer size =     0.38 MiB\n",
            "create_memory: n_ctx = 4352 (padded)\n",
            "llama_kv_cache_unified:      CUDA0 KV buffer size =   850.00 MiB\n",
            "llama_kv_cache_unified: size =  850.00 MiB (  4352 cells,  40 layers,  1/ 1 seqs), K (f16):  425.00 MiB, V (f16):  425.00 MiB\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 2\n",
            "llama_context: max_nodes = 1944\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "llama_context:      CUDA0 compute buffer size =   234.50 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    18.51 MiB\n",
            "llama_context: graph nodes  = 1327\n",
            "llama_context: graph splits = 2\n",
            "Threadpool set to 1 threads and 1 blasthreads...\n",
            "attach_threadpool: call\n",
            "Starting model warm up, please wait a moment...\n",
            "Load Text Model OK: True\n",
            "Chat completion heuristic: ChatML (Phi 4)\n",
            "Embedded KoboldAI Lite loaded.\n",
            "Embedded API docs loaded.\n",
            "======\n",
            "Active Modules: TextGeneration WebSearchProxy\n",
            "Inactive Modules: ImageGeneration VoiceRecognition MultimodalVision NetworkMultiplayer ApiKeyPassword TextToSpeech VectorEmbeddings AdminControl\n",
            "Enabled APIs: KoboldCppApi OpenAiApi OllamaApi\n",
            "Your remote Kobold API can be found at https://bowl-cursor-hero-somehow.trycloudflare.com/api\n",
            "Your remote OpenAI Compatible API can be found at https://bowl-cursor-hero-somehow.trycloudflare.com/v1\n",
            "======\n",
            "Your remote tunnel is ready, please connect to https://bowl-cursor-hero-somehow.trycloudflare.com\n",
            "Token streaming was interrupted or aborted!\n",
            "[Errno 32] Broken pipe\n",
            "\n",
            "[18:06:57] CtxLimit:1776/4096, Amt:429/512, Init:0.00s, Process:2.06s (654.20T/s), Generate:46.25s (9.28T/s), Total:48.31s\n",
            "Generation Aborted\n",
            "Exception ignored in: <module 'threading' from '/tmp/_MEIDqTWu6/threading.pyc'>\n",
            "Traceback (most recent call last):\n",
            "  File \"threading.py\", line 1388, in _shutdown\n",
            "KeyboardInterrupt: \n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#@title <b>v-- Enter your model below and then click this to start Koboldcpp</b>\n",
        "\n",
        "Model = \"https://huggingface.co/bartowski/phi-4-GGUF/blob/main/phi-4-Q6_K_L.gguf\" #@param [\"https://huggingface.co/bartowski/google_gemma-3-27b-it-qat-GGUF/blob/main/google_gemma-3-27b-it-qat-Q2_K.gguf\", \"https://huggingface.co/bartowski/google_gemma-3-27b-it-qat-GGUF/blob/main/google_gemma-3-27b-it-qat-IQ3_XXS.gguf\", \"https://huggingface.co/bartowski/gemma-2-27b-it-GGUF/blob/main/gemma-2-27b-it-Q3_K_M.gguf\", \"https://huggingface.co/bartowski/allura-org_GLM4-9B-Neon-v2-GGUF/blob/main/allura-org_GLM4-9B-Neon-v2-Q8_0.gguf\", \"https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/blob/main/Llama-3.2-3B-Instruct-f16.gguf\", \"https://huggingface.co/bartowski/phi-4-GGUF/blob/main/phi-4-Q6_K_L.gguf\", \"https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/blob/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\"]{allow-input: true}\n",
        "Layers = 99 #@param [99]{allow-input: true}\n",
        "ContextSize = \"4096\" #@param [4096,8192] {allow-input: true}\n",
        "FlashAttention = True #@param {type:\"boolean\"}\n",
        "Multiplayer = False #@param {type:\"boolean\"}\n",
        "FACommand = \"\"\n",
        "MPCommand = \"\"\n",
        "#@markdown <hr>\n",
        "LoadVisionMMProjector = False #@param {type:\"boolean\"}\n",
        "Mmproj = \"https://huggingface.co/koboldcpp/mmproj/resolve/main/llama-13b-mmproj-v1.5.Q4_1.gguf\" #@param [\"https://huggingface.co/koboldcpp/mmproj/resolve/main/llama-13b-mmproj-v1.5.Q4_1.gguf\",\"https://huggingface.co/koboldcpp/mmproj/resolve/main/mistral-7b-mmproj-v1.5-Q4_1.gguf\",\"https://huggingface.co/koboldcpp/mmproj/resolve/main/llama-7b-mmproj-v1.5-Q4_0.gguf\",\"https://huggingface.co/koboldcpp/mmproj/resolve/main/LLaMA3-8B_mmproj-Q4_1.gguf\"]{allow-input: true}\n",
        "VCommand = \"\"\n",
        "#@markdown <hr>\n",
        "LoadImgModel = False #@param {type:\"boolean\"}\n",
        "ImgModel = \"https://huggingface.co/koboldcpp/imgmodel/resolve/main/imgmodel_ftuned_q4_0.gguf\" #@param [\"https://huggingface.co/koboldcpp/imgmodel/resolve/main/imgmodel_ftuned_q4_0.gguf\"]{allow-input: true}\n",
        "SCommand = \"\"\n",
        "#@markdown <hr>\n",
        "LoadSpeechModel = False #@param {type:\"boolean\"}\n",
        "SpeechModel = \"https://huggingface.co/koboldcpp/whisper/resolve/main/whisper-base.en-q5_1.bin\" #@param [\"https://huggingface.co/koboldcpp/whisper/resolve/main/whisper-base.en-q5_1.bin\"]{allow-input: true}\n",
        "WCommand = \"\"\n",
        "#@markdown <hr>\n",
        "LoadTTSModel = False #@param {type:\"boolean\"}\n",
        "TTSModel = \"https://huggingface.co/koboldcpp/tts/resolve/main/OuteTTS-0.2-500M-Q4_0.gguf\" #@param [\"https://huggingface.co/koboldcpp/tts/resolve/main/OuteTTS-0.2-500M-Q4_0.gguf\"]{allow-input: true}\n",
        "WavTokModel = \"https://huggingface.co/koboldcpp/tts/resolve/main/WavTokenizer-Large-75-Q4_0.gguf\" #@param [\"https://huggingface.co/koboldcpp/tts/resolve/main/WavTokenizer-Large-75-Q4_0.gguf\"]{allow-input: true}\n",
        "TTSCommand = \"\"\n",
        "#@markdown <hr>\n",
        "LoadEmbeddingsModel = False #@param {type:\"boolean\"}\n",
        "EmbeddingsModel = \"https://huggingface.co/yixuan-chia/snowflake-arctic-embed-s-GGUF/resolve/main/snowflake-arctic-embed-s-Q4_0.gguf\" #@param [\"https://huggingface.co/yixuan-chia/snowflake-arctic-embed-s-GGUF/resolve/main/snowflake-arctic-embed-s-Q4_0.gguf\"]{allow-input: true}\n",
        "ECommand = \"\"\n",
        "#@markdown <hr>\n",
        "#@markdown This enables saving stories directly to your google drive. You will have to grant permissions, and then you can access the saves from the \"KoboldCpp Server Storage\" option.\n",
        "AllowSaveToGoogleDrive = True #@param {type:\"boolean\"}\n",
        "SavGdriveCommand = \"\"\n",
        "#@markdown <hr>\n",
        "#@markdown Only select the following box if regular cloudflare tunnel fails to work. It will generate an inferior localtunnel tunnel, which you can use after entering a password.\n",
        "MakeLocalTunnelFallback = False #@param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "if not os.path.isfile(\"/opt/bin/nvidia-smi\"):\n",
        "  raise RuntimeError(\"⚠️Colab did not give you a GPU due to usage limits, this can take a few hours before they let you back in. Check out https://lite.koboldai.net for a free alternative (that does not provide an API link but can load KoboldAI saves and chat cards) or subscribe to Colab Pro for immediate access.⚠️\")\n",
        "\n",
        "if AllowSaveToGoogleDrive:\n",
        "  print(\"Attempting to request access to save to your google drive...\")\n",
        "  try:\n",
        "    from google.colab import drive\n",
        "    import os, json\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "      raise RuntimeError(\"Google Drive mount failed. Please grant permissions and try again.\")\n",
        "    kcppdir = '/content/drive/MyDrive/koboldcpp_data'\n",
        "    os.makedirs(kcppdir, exist_ok=True)\n",
        "    savedatapath = os.path.join(kcppdir, \"koboldcpp_save_db.jsondb\")\n",
        "    if not os.path.exists(savedatapath):\n",
        "      settings_data = {}\n",
        "      with open(savedatapath, \"w\") as json_file:\n",
        "        json.dump(settings_data, json_file, indent=4)\n",
        "      print(f\"Created new koboldcpp_save_db.jsondb at {savedatapath}\")\n",
        "    else:\n",
        "      print(f\"Loading saved data at {savedatapath}\")\n",
        "    SavGdriveCommand = f\" --savedatafile {savedatapath}\"\n",
        "  except Exception as e:\n",
        "    print(f\"⚠️ Error: {e}\")\n",
        "    print(\"Please ensure you grant Google Drive permissions and try again.\")\n",
        "\n",
        "%cd /content\n",
        "if Mmproj and LoadVisionMMProjector:\n",
        "  VCommand = \"--mmproj vmodel.gguf\"\n",
        "else:\n",
        "  SCommand = \"\"\n",
        "if ImgModel and LoadImgModel:\n",
        "  SCommand = \"--sdmodel imodel.gguf --sdthreads 4 --sdquant --sdclamped\"\n",
        "else:\n",
        "  SCommand = \"\"\n",
        "if SpeechModel and LoadSpeechModel:\n",
        "  WCommand = \"--whispermodel wmodel.bin\"\n",
        "else:\n",
        "  WCommand = \"\"\n",
        "if TTSModel and WavTokModel and LoadTTSModel:\n",
        "  TTSCommand = \"--ttsmodel ttsmodel.bin --ttswavtokenizer ttswavtok.bin --ttsgpu\"\n",
        "else:\n",
        "  TTSCommand = \"\"\n",
        "if EmbeddingsModel and LoadEmbeddingsModel:\n",
        "  ECommand = \"--embeddingsmodel emodel.bin\"\n",
        "else:\n",
        "  ECommand = \"\"\n",
        "if FlashAttention:\n",
        "  FACommand = \"--flashattention\"\n",
        "else:\n",
        "  FACommand = \"\"\n",
        "if Multiplayer:\n",
        "  MPCommand = \"--multiplayer\"\n",
        "else:\n",
        "  MPCommand = \"\"\n",
        "\n",
        "!echo Downloading KoboldCpp, please wait...\n",
        "!wget -O dlfile.tmp https://kcpplinux.concedo.workers.dev && mv dlfile.tmp koboldcpp_linux\n",
        "!test -f koboldcpp_linux && echo Download Successful || echo Download Failed\n",
        "!chmod +x ./koboldcpp_linux\n",
        "!apt update\n",
        "!apt install aria2 -y\n",
        "# simple fix for a common URL mistake\n",
        "if \"https://huggingface.co/\" in Model and \"/blob/main/\" in Model:\n",
        "    Model = Model.replace(\"/blob/main/\", \"/resolve/main/\")\n",
        "!aria2c -x 10 -o model.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $Model\n",
        "if VCommand:\n",
        "  !aria2c -x 10 -o vmodel.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $Mmproj\n",
        "if SCommand:\n",
        "  !aria2c -x 10 -o imodel.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $ImgModel\n",
        "if WCommand:\n",
        "  !aria2c -x 10 -o wmodel.bin --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $SpeechModel\n",
        "if TTSCommand:\n",
        "  !aria2c -x 10 -o ttsmodel.bin --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $TTSModel\n",
        "  !aria2c -x 10 -o ttswavtok.bin --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $WavTokModel\n",
        "if ECommand:\n",
        "  !aria2c -x 10 -o emodel.bin --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $EmbeddingsModel\n",
        "\n",
        "if MakeLocalTunnelFallback:\n",
        "  import urllib\n",
        "  print(\"Trying to use LocalTunnel as a fallback tunnel (not so good)...\")\n",
        "  ltpw = urllib.request.urlopen('https://loca.lt/mytunnelpassword').read().decode('utf8').strip(\"\\n\")\n",
        "  !nohup npx --yes localtunnel --port 5001 > lt.log 2>&1 &\n",
        "  !sleep 8\n",
        "  print(\"=================\")\n",
        "  print(\"(LocalTunnel Results)\")\n",
        "  !cat lt.log\n",
        "  print(f\"Please open the above link, and input the password '{ltpw}'\\nYour KoboldCpp will start shortly...\")\n",
        "  print(\"=================\")\n",
        "  !sleep 10\n",
        "!./koboldcpp_linux model.gguf --usecublas 0 mmq --chatcompletionsadapter AutoGuess --multiuser --gpulayers $Layers --contextsize $ContextSize --websearch --quiet --remotetunnel $FACommand $MPCommand $VCommand $SCommand $WCommand $TTSCommand $ECommand $SavGdriveCommand\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}